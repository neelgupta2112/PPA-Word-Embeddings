{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c42c4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72ff56ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_parquet(\"../Data/ppa_corpus_2025-02-03_1308/keywords_and_top_1000.parquet\")\n",
    "df = pd.read_csv(\"../Data/ppa_corpus_2025-02-03_1308/ppa_keyword_results_1000.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed7dd0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>work_id</th>\n",
       "      <th>poetic_form</th>\n",
       "      <th>top_1000_word</th>\n",
       "      <th>counts</th>\n",
       "      <th>contexts</th>\n",
       "      <th>page_text</th>\n",
       "      <th>spelling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A01224.1</td>\n",
       "      <td>A01224</td>\n",
       "      <td>NaN</td>\n",
       "      <td>right</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A01224.1</td>\n",
       "      <td>A01224</td>\n",
       "      <td>NaN</td>\n",
       "      <td>excellent</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A01224.1</td>\n",
       "      <td>A01224</td>\n",
       "      <td>NaN</td>\n",
       "      <td>french</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A01224.1</td>\n",
       "      <td>A01224</td>\n",
       "      <td>NaN</td>\n",
       "      <td>non</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A01224.1</td>\n",
       "      <td>A01224</td>\n",
       "      <td>NaN</td>\n",
       "      <td>latin</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45429237</th>\n",
       "      <td>yale.39002088447587.00000496</td>\n",
       "      <td>yale.39002088447587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>words</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45429238</th>\n",
       "      <td>yale.39002088447587.00000496</td>\n",
       "      <td>yale.39002088447587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phrases</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45429239</th>\n",
       "      <td>yale.39002088447587.00000496</td>\n",
       "      <td>yale.39002088447587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>correct</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45429240</th>\n",
       "      <td>yale.39002088447587.00000496</td>\n",
       "      <td>yale.39002088447587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>peace</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45429241</th>\n",
       "      <td>yale.39002088447587.00000496</td>\n",
       "      <td>yale.39002088447587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>end</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45429242 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               page_id              work_id poetic_form  \\\n",
       "0                             A01224.1               A01224         NaN   \n",
       "1                             A01224.1               A01224         NaN   \n",
       "2                             A01224.1               A01224         NaN   \n",
       "3                             A01224.1               A01224         NaN   \n",
       "4                             A01224.1               A01224         NaN   \n",
       "...                                ...                  ...         ...   \n",
       "45429237  yale.39002088447587.00000496  yale.39002088447587         NaN   \n",
       "45429238  yale.39002088447587.00000496  yale.39002088447587         NaN   \n",
       "45429239  yale.39002088447587.00000496  yale.39002088447587         NaN   \n",
       "45429240  yale.39002088447587.00000496  yale.39002088447587         NaN   \n",
       "45429241  yale.39002088447587.00000496  yale.39002088447587         NaN   \n",
       "\n",
       "         top_1000_word  counts contexts page_text spelling  \n",
       "0                right       1      NaN       NaN      NaN  \n",
       "1            excellent       1      NaN       NaN      NaN  \n",
       "2               french       1      NaN       NaN      NaN  \n",
       "3                  non       3      NaN       NaN      NaN  \n",
       "4                latin       3      NaN       NaN      NaN  \n",
       "...                ...     ...      ...       ...      ...  \n",
       "45429237         words       1      NaN       NaN      NaN  \n",
       "45429238       phrases       1      NaN       NaN      NaN  \n",
       "45429239       correct       1      NaN       NaN      NaN  \n",
       "45429240         peace       1      NaN       NaN      NaN  \n",
       "45429241           end       1      NaN       NaN      NaN  \n",
       "\n",
       "[45429242 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fcdf410",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['poetic_form'] = np.where(\n",
    "    df['poetic_form'].isna(),\n",
    "    df['top_1000_word'],\n",
    "    df['poetic_form']\n",
    ")\n",
    "\n",
    "df['spelling'] = np.where(\n",
    "    df['spelling'].isna(),\n",
    "    df['top_1000_word'],\n",
    "    df['spelling']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae176849",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"../Data/ppa_corpus_2025-02-03_1308/ppa_keyword_results_1000_edited.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94768bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_usage_representations(text, tokenizer, model, device=\"cpu\", skip_stopwords=True):\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    input_ids = encoded[\"input_ids\"].to(device)\n",
    "    attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "    offsets = encoded[\"offset_mapping\"][0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        hidden_states = output.hidden_states  # (layers, batch, seq_len, hidden_size)\n",
    "\n",
    "    all_layers = torch.stack(hidden_states, dim=0)  # (layers, batch, seq_len, hidden)\n",
    "    summed = all_layers.sum(dim=0)[0]  # (seq_len, hidden_size)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    special_tokens = set(tokenizer.all_special_tokens)\n",
    "\n",
    "    usage_vectors = []\n",
    "    current_word = \"\"\n",
    "    current_vecs = []\n",
    "    current_start = None\n",
    "    current_end = None\n",
    "\n",
    "    STOPWORDS = {\n",
    "       \"the\", \"and\", \"for\", \"but\", \"with\", \"that\", \"this\", \"from\", \"not\",\n",
    "      \"you\", \"are\", \"was\", \"were\", \"have\", \"has\", \"had\", \"she\", \"he\", \"they\",\n",
    "       \"his\", \"her\", \"its\", \"our\", \"their\", \"will\", \"would\", \"can\", \"could\"\n",
    "    }\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        # skip special tokens\n",
    "        if token in special_tokens:\n",
    "            continue\n",
    "\n",
    "        # detect new word\n",
    "        is_new_word = token.startswith(\"Ġ\") or i == 0 or token.startswith(\"Ċ\")\n",
    "\n",
    "        if is_new_word and current_word:\n",
    "            # aggregate previous word\n",
    "            word_vec = torch.stack(current_vecs).mean(dim=0)\n",
    "\n",
    "            # strip punctuation + lowercase\n",
    "            clean_word = current_word.lower().strip(string.punctuation)\n",
    "\n",
    "            if clean_word and (not skip_stopwords or clean_word not in STOPWORDS):\n",
    "                usage_vectors.append({\n",
    "                    \"word\": clean_word,\n",
    "                    \"vector\": word_vec.cpu(),\n",
    "                    \"char_start\": current_start,\n",
    "                    \"char_end\": current_end\n",
    "                })\n",
    "\n",
    "            current_vecs = []\n",
    "\n",
    "        if is_new_word:\n",
    "            current_word = token.lstrip(\"ĠĊ\")\n",
    "            current_start = offsets[i][0].item()\n",
    "            current_end = offsets[i][1].item()\n",
    "            current_vecs.append(summed[i])\n",
    "        else:\n",
    "            current_word += token\n",
    "            current_end = offsets[i][1].item()\n",
    "            current_vecs.append(summed[i])\n",
    "\n",
    "    # Handle last word\n",
    "    if current_word:\n",
    "        word_vec = torch.stack(current_vecs).mean(dim=0)\n",
    "        clean_word = current_word.lower().strip(string.punctuation)\n",
    "        if clean_word and (not skip_stopwords or clean_word not in STOPWORDS):\n",
    "            usage_vectors.append({\n",
    "                \"word\": clean_word,\n",
    "                \"vector\": word_vec.cpu(),\n",
    "                \"char_start\": current_start,\n",
    "                \"char_end\": current_end\n",
    "            })\n",
    "\n",
    "    return usage_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c869214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_iter(pages_file):\n",
    "   # Yield pages one at a time from gzipped JSON lines file for memory efficiency\n",
    "   with gzip.open(pages_file, 'rt', encoding='utf-8') as fh:\n",
    "       for line in fh:\n",
    "           yield json.loads(line)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92bed707",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_id = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dedd47",
   "metadata": {},
   "source": [
    "Testing the Tokenizer against OCRd text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9184bbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 4724.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '[1]', 'tags': ['dedication'], 'id': 'A01224.1', 'order': 1, 'text': '\\nTo the Right excellent and most honorable Ladie, the Ladie Marie, Countesse of Pembroke.\\nVOi, pia nympha, tuum, quem tolse la morte, Philippū,\\nAEdentem llenas coelestis melle palabras.\\nItalicum lumen, flowre of Fraunce, splendor Iberus,\\nItalicus Tasso, French Salust, Boscan Iberus,\\n〈 in non-Latin alphabet 〉 Virgil, 〈 in non-Latin alphabet 〉,\\nGreekish Homer, tanto lati iunguntur 〈 in non-Latin alphabet 〉.\\nYour Honors most affectionate. Abraham Fraunce.\\n\\n\\n\\n\\n', 'work_id': 'A01224'}\n",
      "{'label': '[10]', 'tags': ['book'], 'id': 'A01224.10', 'order': 10, 'text': \"Boscan 3. Booke.\\nLos altares delante estauan puestos,\\nArdiendo encima d'ellos toda Arabia.\\n\\n\\nCap. 5. Of the Metonymia of the adiunct.\\nA Metonymia of the adiunct, is, when by the adiunct we expresse the subiect. So the names of vertues & vices are vsed for vertuous or vicious men: the signe for the thing which it doth signifie: the adiūct of the time for the things subiect, &c.\\nHomer. N. Il. \\n〈 in non-Latin alphabet 〉.\\n\\nSo in \\n6. Odyss.\\n〈 in non-Latin alphabet 〉.\\n\\nfor Alcinous himself.\\nV. Ae. 1. \\nQuis genus Aeneadûm, quis Troiae nesciat vrbē,\\nVirtutésque, virósque, & tanti incendia belli?\\n\\nAeneid. 1. \\nHaud aliter puppés{que} tua, pubés{que} tuorum, &c.\\nAeneid. 1. \\nAspera tum positis mitescent saecula bellis.\\n\\nGeorg. 1. \\nImpia{que} aternam timuerunt saecula noctem.\\n\\nAglog. 1. \\n—libet Partho torquere Cydonia cornis Spicula.\\n\\nSir Philip Sidney 5. Musidorus to Euarchus.\\nHowsoeuer it bee, my death shall triumph ouer thy crueltie.\\n\\n\", 'work_id': 'A01224'}\n",
      "{'label': '[100]', 'tags': ['book'], 'id': 'A01224.100', 'order': 100, 'text': '\\n〈 in non-Latin alphabet 〉.\\n\\n〈 in non-Latin alphabet 〉.\\nThis is the Prosopopoeia of Peleus, which is thus left off;\\n〈 in non-Latin alphabet 〉.\\nVirgil. Aeneid. 1. Of Aeneas.\\n—& dictis moerentia pectora mulcet.\\nO socij (neque enim ignari sumus antemalorum)\\nO passi grauiora! dabit Deus his quoque finem.\\nVos & Scyllaeam rabiem, penitus{que} sonantes\\nAccestis scopulos: vos & Cyclopea saxa\\nExperti: reuocate animos, moestùmque timorem\\nMittite: forsan & haec olim meminisse iuuabit.\\nPer varios casus, per tot discrimina rerum\\nTendimus in Latium, sedes vbi fata quietas\\nOstendunt: illie fas regna resurgere Troiae:\\nDurate, & vosinet rebus seruate secundis.\\nTalia voce refert, curisque ingentibus ager,\\nSpem vultu simulat, premit altum corde dolorem.\\nSo of Achemenides 3. Aeneid.\\nCùm subitò è syluis, macie confecta suprema\\nIgnoti noua forma viri, miserandáque cultu\\nProcedit, suppléxque manus ad littora tendit.\\nRespicimus: dira illuuies, immissa{que} barba,\\nConsertum tegimen spinis: at caetera Graius,\\nEt quondam patrijs ad Troiam missus in armis.\\nIs{que} vbi Dardanios habitus & Troia vidit\\nArma procul: paulùm aspectu conterritus haesit,\\nContinuitque gradum: mox sese ad littora praeceps\\nCum fletu, precibúsque tulit. Per sidera testor,\\nPer superos, atque hoc coeli spirabile lumen,\\nTollite me Teucri, quascunque abducite terras.\\nHoc sat erit. Scio me Danais è classibus vnum,\\n', 'work_id': 'A01224'}\n",
      "{'label': '[101]', 'tags': ['book'], 'id': 'A01224.101', 'order': 101, 'text': '\\nEt bello Iliacos fateor perijsse penates:\\nPro quo, sisceleria tanta est iniuria nostra,\\nSpargite me influctus, vastóque immergite ponto.\\nSi pereo, hominum manibus perijsse iuuabit.\\nDixerat, & genua amplexus genibisque volutans\\nHarebat.\\n\\nSir Philip Sydney 1. Of Musidorus clad in shepheards weedes.\\nShe might percetue a farre off one cōming towards her in the apparaile of a shepheard, with his armes hanging down, going a kinde of languishing pace, with his eyes sometimes cast vp to heauen, as though his fancie straue to moūt vp higher; somtimes thrown down to the groūd, as if the earth could not beare the burden of his paines: at length she heard him with a lamentable tune sing these few verses:\\n\\nCome shepheards weedes, become your masters mind,\\nYeeld outward shew, what inward change he tries,\\nNor be abasht, since such a guest you find,\\nWhose strongest hope in your weake comfort lies.\\nCome shepheards weeds, attend my wofull cries,\\nDisuse your selues from sweet Menalcaes voyce,\\nFor other be those tunes which sorowe ties,\\nFrom those cleare notes which freelie may reioyce.\\nThen powre out plaints, and in one word say this,\\nHelples his plaint who spoyles himselfe of blisse.\\n\\nAnd hauing ended, she might see him strike himselfe vppon the breast, vttering\\n\\n\\n', 'work_id': 'A01224'}\n",
      "{'label': '[102]', 'tags': ['book'], 'id': 'A01224.102', 'order': 102, 'text': \"\\nthese words. O miserable wretch, whether doothy destinies guide thee?\\n\\nTorquat. Tass. 3. Godfrey to his Souldiers.\\nIl saggio Capitan, che l'ardimento\\nSolite loro, in essi hor non comprende:\\nCerca con lieto volto, è conparole\\nCome gli rassicuri, è riconsole.\\nO per mille perigli, è mille affanni\\nMeco passati in quelle parti, e'n queste,\\nCampion di Dio, ch'à ristorare i danni\\nDe la Christiana sua fede nasceste,\\nVoi, che l'arme di Persia, è i Greci ing anni\\nE i monti, è i mari, el verno, è le tempeste,\\nDe la fame i disagi, & de la sete\\nSuperaste: voi dunque hora temete?\\nDun{que} il signor, che m'indirizza, è moue\\nGià conosciuto in caso assai più •io,\\nNon v'assicura? quasi hor volga altroue\\nLa man de la clementia, e'lguardo pio.\\nTosto vn difia, che rimembrar vi gious\\nGli scorsi affanni, è sciorre i voti à dio.\\nHor durate magnanimi, è voi stessi\\nSerbate, prego, 〈◊〉 i prosperi successi.\\nCon queste detti le smarxite menti\\nConsola, è con sereno è lieto aspetto:\\nMà preme mille cure egro è dolenti\\nAltamente riposte in mezz' alpetto.\\n\\nSalust. Iudith. 2, God to Moses.\\n\\nVniour que cet Hebrieu sur Oreb menoit paitre\\nLes laineuses brebis de son beau pere Ietre,\\nIl voit, tout effraié comme vn rougeatre feu\\nSans amorce s'espend en vn halier toufu:\\nD'où sort auec grand bruit vne telle parole\\n\\n\\n\", 'work_id': 'A01224'}\n",
      "{'label': '[103]', 'tags': ['book'], 'id': 'A01224.103', 'order': 103, 'text': \"\\nQui soudain fait trembler & la terre & le polo.\\n\\n\\nIesuis cil qui seul est, seul fut, & seul sera:\\nCil qui de rien fit tout, & qui fort reduira\\nS'il veut, le tout en tien. Iesuis le grand, le iuste,\\nLe beau, le bon, le saint, dont la dextre robuste\\nBalance l'vniuers. Iesuis le tout-puissant,\\nQu' Abram seul adoroit: qui d'vn fleau puissant\\nDestrui mes enemis, & qui benin, fa grace\\nA ceux qui m'ont pour Dieu voire à toute leur race.\\nSi doncques mon vouloir. Va-ten, despeche toi,\\nFai scauoir de ma part à ce profane roy,\\nQui tient les tours de Memphe, & la grasse campagne,\\nQue le Nil debordé de son flot riche bagne,\\nQu'il assranchisse Isac. Et de peur que le roy\\nIncredule ne mettent en doute ton enuoy,\\nIeveus que sur les fleurs ta houlette alongee,\\nPour confirmer ta charge, en serpent soit changee.\\n\\n\\nBoscan. 3. of Hero.\\n\\nElla mouida entonces con mas san̄a,\\nNise dexolleuar por donde el quiso,\\nNisobre el manto le sufrio la mano,\\nMas buelta sobre si congraue gesto,\\nSemeiantes palabras d'estas dizo:\\n\\n\\nO hombre que veniste por mal tuyo\\nA este templo, a des lustrar mi houra,\\nSin entender quan gran locura emprendes.\\nNo sabes tu que soy sierua de Venus,\\nY virgin, y por virgen que la siruo?\\nNo sabes tulos hombres de mi sangre\\nQue ti castigaran, si saben esto?\\n\\n\\n\", 'work_id': 'A01224'}\n",
      "{'label': '[104]', 'tags': ['book'], 'id': 'A01224.104', 'order': 104, 'text': '\\nY no sabes tambien que estoy criada\\nEngrande encerramiento con mis padres?\\nY que nunca salit querer llegar\\nAllecho virginal, es cosa dura:\\nVete y iamas parezcas do estuuiere.\\n\\n\\nEsto dicho callà como vencida,\\nConlagrimas venidas à los oyos\\nPero bueltas atras luego en vn punto.\\n\\nBy this figure wee sometimes make dumme and senceles things speake.\\nSo Homer. Τ. Iliad. makes Xanthus Achilles horse speake.\\n\\n〈 in non-Latin alphabet 〉, &c.\\nAnd endeth thus.\\n\\n\\nVirgil. 3. Aeneid. maketh the Images of the Troian Godds appeare to Aeneas in sleepe, expounding the Oracles, instructing him in his way, &c. And there, the chief of the Harpies, Celeno speaketh thus.\\nVna in praecelsa consedit rupe Celeno,\\nInfoelix vates, rupitque hanc pectore vocem.\\nBellum etiam pro caede boum, stratis{que} iuuencis,\\nLaomedontiadae, bellumne inferre paratis?\\nEt patrio insontes Harpyas pellere regno?\\nAccipite ergo animis, atque haec mea figite dicta.\\n\\n\\n', 'work_id': 'A01224'}\n",
      "{'label': '[105]', 'tags': ['book'], 'id': 'A01224.105', 'order': 105, 'text': '\\nQuae Phaebe pater omnipotens, mihi Phaebus Apollo\\nPraedixit, vobis furiarum ego maxima pando.\\nItaliam cursu petitis, ventis{que} vocatis\\nIbitis Italianm portus{que} intrare licebit:\\nSed non ante datam cangetis maenibus vrbem,\\nQuàm vos dira fames, nostra{que} iniuria caedis\\nAmbesas subigat malis absumere mensas.\\nDixit & in syluas pennis ablata refugit.\\n\\nSir Philip Sydney 5. Philanax to Euarchus.\\nAnd giue me leaue, excellent Euarchus, to say it, I am but the representer of all the late florishing Arcadia, which now with mine eyes doth weepe, with my tongue doth complaine, with my knees doth lay it selfe at your feete, which neuer haue bin vnreadie to carie you to the vertuous protecting of innocents. Imagine, vouchsafe to imagine, most wise & good King, that here is before your eyes the pitifull spectacle of a most dolorous ending tragedie. Wherein I doo but play the part of al this now miserable prouince, which being spoyled of her guide, doth lie like a shipp without a Pilote, tumbling vp and downe in the vncertaine waues, till it either runne it selfe vpon the rocke of selfe diuision, or bee ouerthrowne by the stormie windes of forraine force. Arcadia\\n\\n\\n', 'work_id': 'A01224'}\n",
      "{'label': '[106]', 'tags': ['book'], 'id': 'A01224.106', 'order': 106, 'text': \"\\nfinding her selfe in these desolate termes, doth speake, and I speake for her, to thee not vainlie puissant Prince. That since now she is not onlie robbed of the naturall support of her Lord, but so sodainlie robbed, that she hath not breathing time, to stand for her safetie: so vnfortunatelie, that it dooth appale her minde though shee had leisure, and so mischieuouslie, that it doth exceed both the sodaines and vnfortunatenes of it: thou wilt bend thine armes vnto her, and as a man, take compassion of mankind: as a vertuous man, chastice most abominable vice; and as a Prince protect a people. &c.\\n\\nTorquat. Tass. 20. Emiren to the Souldiers.\\nCredi, dicea, che la tua patria spieghi\\nPer la mia lingua, in tai parole, i preghi.\\nGuarda tù le mie leggi, e i sacri tempi:\\nFà, che io del sangue mio, non bagni, e laui,\\nAssicura le vergini da gli empi,\\nE i sepolcri, e le cineri de gli aui.\\nA te, piaigendo i lor passati tempi,\\nMostran la bianca chioma i vecchi graui,\\nA te la moglie le mammelle, e'l petto:\\nLe cune, e i faghi, e'l marital suo letto.\\n16. He makes the Parrat to speak.\\n\\nVolafràgli altrivu, che le piume hà sparte\\nDi color vari: & è purpureo il rostro:\\n\\n\\n\", 'work_id': 'A01224'}\n",
      "{'label': '[107]', 'tags': ['book'], 'id': 'A01224.107', 'order': 107, 'text': \"\\nE lingua suoda in guisa varia: e parte\\nLa voce si, ch' assembra il sermon nostro.\\nQuest'iui alhor continuo con arte\\nTanta il parlar che fà mirabil mostro.\\nTaequero gli altri ad ascoltarlo intenti:\\nEfermaro i susurri in aria i venti.\\nDeh mira, egli canto, spuntar la rosa\\nDal verde suo: modesta, e verginella,\\nChe mezzo aperta ancora, e mezzo ascosa:\\nQuanto si mostra men, tant' è piu bella,\\nEcco poi nudo ilsen già baldanzosa\\nDispiega. Ecco poi langue: e non par quella:\\nQuella non par, che desiata innanti\\nFu da mille donzelli, e mille amanti.\\nCest trapassa, al trapassar d'un giorno,\\nDa la vita mortale il fiore, e'l verde,\\nNe perche faccia indietro Aprilritorno:\\nSi rinfiora ella mai, ne si rinuerde.\\nCogliam la rosa in su'l mattino adorno\\nDi questo di, che tosto il seren perde:\\nCogliam d' amor la rosa: amiamo hor quando\\nEsser si puote riamato amando.\\nTacque: e concorde de gli augelli il choro\\nQuasi approuando.\\n\\nSalust. 2. Semaine. That of the Serpent.\\nIngress.\\nLe dragon pur forcer l'humaine forteresse, &c.\\nThe Speach.\\nEue second honneur de ce grand vniuers, &c.\\nRegress.\\nAuec l'ayr de ces mots l'infidele viper\\nSouffle vn air venemeux au sein de nostre mere, &c.\\n\\n2. Semaine. 6. Of God seeing the tower of Babel.\\nIngress. Quoy voyant l'eternel, &c.\\nSpeach. Voyez, dit il, ices nains, voyez ceste racaillè, &c.\\nRegress. Celadit, tout soudain sespand confusement,\\nVn ie ne scay quel brust par tout le bastement.\\n\\n\\n\", 'work_id': 'A01224'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Finding a page with some clear OCR errors\n",
    "\n",
    "TARGET_COLLECTIONS = {\"Literary\", \"Linguistic\"}\n",
    "with open(\"Data/ppa_corpus_2025-02-03_1308/ppa_metadata.json\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "metadata_index = {\n",
    "    entry[\"work_id\"]: entry for entry in metadata\n",
    "    if \"collections\" in entry and any(c in TARGET_COLLECTIONS for c in entry[\"collections\"])\n",
    "\n",
    "}\n",
    "\n",
    "for example in tqdm(islice(page_iter(\"Data/ppa_corpus_2025-02-03_1308/ppa_pages.jsonl.gz\"), 10)):\n",
    "    wid = example.get(\"work_id\")\n",
    "    if wid in metadata_index:\n",
    "        print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89bbc4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## found in siddarth's dataframe row 1463\n",
    "\n",
    "example_text = \"The Epi≈øodes of a Tragedy, ought\\nto be infinitely fhorter, than thofe of an Epick Poem,\\nfor thefe two reafons: The firft is, That Tragedy is\\nmuch shorter, fince 'tis confined to one s: The firft is, That Tragedy is\\nmuch shorter, fince 'tis confined to one Courfe of the\\nSun, but an Epick Poem has no fet time. The fe-\\ncond is, becaufe Tragedy is a repre≈øentation, and\\nEpick Poem a recita 'e\\nSun, but an Epick Poem has no fet time. The fe-\\ncond is, becaufe Tragedy is a repre≈øentation, and\\nEpick Poem a recitation, and this is the reafon why\\nit ought to be extended and amplified by its E-\\npifod\"\n",
    "\n",
    "\n",
    "\n",
    "cleaned_text = \"The Episodes of a Tragedy, ought to be infinitely shorter, than those of an Epic Poem, for these two reasons: The first is, That Tragedy is much shorter, since 'tis confined to one course of the Sun, but an Epic Poem has no set time. The second is, because Tragedy is a representation, and Epic Poem a recitation, and this is the reason why it ought to be extended and amplified by its Episode\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a2a18de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['episodes', 'of', 'a', 'tragedy', 'ought', 'to', 'be', 'infinitely', 'shorter', 'than', 'those', 'of', 'an', 'epic', 'poem', 'these', 'two', 'reasons', 'first', 'is', 'tragedy', 'is', 'much', 'shorter', 'since', 'tis', 'confined', 'to', 'one', 'course', 'of', 'sun', 'an', 'epic', 'poem', 'no', 'set', 'time', 'second', 'is', 'because', 'tragedy', 'is', 'a', 'representation', 'epic', 'poem', 'a', 'recitation', 'is', 'reason', 'why', 'it', 'ought', 'to', 'be', 'extended', 'amplified', 'by', 'episode']\n"
     ]
    }
   ],
   "source": [
    "## what the associated word reconstruction from the clean text looks like\n",
    "\n",
    "cleaned_list = extract_usage_representations(cleaned_text, tokenizer, model)\n",
    "\n",
    "\n",
    "cleaned_words = [entry[\"word\"] for entry in cleaned_list]\n",
    "\n",
    "print(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a220b099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['epiâīīã¸odes', 'of', 'a', 'tragedy', 'oughtċto', 'be', 'infinitely', 'fhorter', 'than', 'thofe', 'of', 'an', 'epick', 'poem,ċfor', 'thefe', 'two', 'reafons', 'firft', 'is', 'tragedy', 'isċmuch', 'shorter', 'fince', 'tis', 'confined', 'to', 'one', 's', 'firft', 'is', 'tragedy', 'isċmuch', 'shorter', 'fince', 'tis', 'confined', 'to', 'one', 'courfe', 'of', 'theċsun', 'an', 'epick', 'poem', 'no', 'fet', 'time', 'fe-ċcond', 'is', 'becaufe', 'tragedy', 'is', 'a', 'repreâīīã¸entation', 'andċepick', 'poem', 'a', 'recita', 'eċsun', 'an', 'epick', 'poem', 'no', 'fet', 'time', 'fe-ċcond', 'is', 'becaufe', 'tragedy', 'is', 'a', 'repreâīīã¸entation', 'andċepick', 'poem', 'a', 'recitation', 'is', 'reafon', 'whyċit', 'ought', 'to', 'be', 'extended', 'amplified', 'by', 'e-ċpifod']\n"
     ]
    }
   ],
   "source": [
    "## what the associated word reconstruction from the example text looks like\n",
    "\n",
    "example_list = extract_usage_representations(example_text, tokenizer, model)\n",
    "\n",
    "\n",
    "example_words = [entry[\"word\"] for entry in example_list]\n",
    "\n",
    "print(example_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c5d2999c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'epic',\n",
       " 'vector': tensor([ 3.4044e+01, -4.8486e+01, -6.2698e+01,  3.9932e+01,  9.5143e+00,\n",
       "         -8.2894e+01,  3.8096e+01,  5.1511e+00,  5.8480e+01, -3.4456e+01,\n",
       "          6.3046e+01, -2.3412e+00, -7.3447e+01,  1.0089e+02, -1.1287e+02,\n",
       "          1.0532e+02, -3.5047e+01, -3.2392e+00, -7.6470e+01, -6.7887e+01,\n",
       "          4.9302e+01, -2.5757e+01, -7.3687e+00, -1.0590e+02,  1.0167e+01,\n",
       "          1.4516e+01,  3.6064e+01,  4.9502e+01, -1.0230e+02, -4.6884e+01,\n",
       "         -8.0251e+00, -4.1092e+02, -9.4452e+00, -1.0868e+02, -4.6609e+00,\n",
       "         -1.9703e+01, -2.7092e+01, -2.3705e+01, -2.5462e+01,  5.7673e+01,\n",
       "         -6.6784e+00, -3.5815e+01, -2.6397e+01,  4.8072e+01, -4.7943e+01,\n",
       "          4.8018e+01, -2.2998e+01,  8.3516e+01, -3.9930e+01,  7.0841e+01,\n",
       "          1.5261e+01, -4.7520e+01, -2.1284e+01,  5.0030e+01, -3.1565e+01,\n",
       "         -5.6257e+01,  1.0419e+02, -4.8078e+01,  1.5623e+01,  4.9759e+01,\n",
       "         -5.9049e+00, -5.6126e+00,  1.6446e+01,  3.6628e+01,  4.0708e+01,\n",
       "          1.4827e+01,  7.6532e+01,  1.1400e+03,  4.6218e+01, -5.5890e+01,\n",
       "          2.1215e+01,  6.6348e+00, -1.0213e+02, -3.1619e+01,  3.0942e+01,\n",
       "         -1.7778e+02, -2.8221e+01, -8.8159e+01,  6.9884e+01, -1.9611e+02,\n",
       "          3.0629e+01, -3.7929e+01,  1.1710e+01,  7.0124e+00, -4.4665e+00,\n",
       "          9.7232e+01, -6.1115e+00,  8.7731e+01,  7.4566e+01,  7.2089e+01,\n",
       "         -8.3346e+01, -6.8643e+01,  1.2808e+01, -2.8229e+01, -6.6803e+00,\n",
       "          4.7982e+01,  5.7562e+01,  4.5433e-01,  1.8147e+00,  1.8378e+00,\n",
       "         -1.4489e+01,  1.6192e+02,  2.6794e+01, -5.7943e+01, -1.1216e+02,\n",
       "          9.2459e+01, -5.7045e+01,  3.9899e+01,  5.9266e+01, -1.7104e+01,\n",
       "         -9.4661e+01, -2.0429e+01, -8.3546e+01, -1.1993e+02, -1.3575e+01,\n",
       "          1.5858e+01, -1.3720e+02,  1.7943e+01,  2.1600e+01, -1.0927e+01,\n",
       "         -3.3450e+01,  3.2587e+01, -2.0452e+01,  9.3412e+01, -2.7207e+01,\n",
       "         -1.4514e+01,  2.4082e+01, -1.8956e+00, -4.4388e+01, -8.5175e+00,\n",
       "         -3.8242e+01, -1.4595e+00,  1.6458e+02,  1.9747e+01,  2.4025e+01,\n",
       "          3.7445e+01,  2.2766e+01,  6.9903e+01,  3.7433e+01, -1.2891e-01,\n",
       "          6.7060e+01,  4.5252e+01, -5.1933e+02,  8.3003e+01,  2.7635e+01,\n",
       "          6.4602e+01,  6.0020e+01, -6.3707e+00,  3.3982e+01,  3.0889e+01,\n",
       "         -1.7346e+02, -5.8174e+01, -2.6232e+01,  8.2097e+01,  1.1927e+02,\n",
       "         -7.0412e+01, -6.6091e+01,  3.6091e+01, -7.4125e+01,  6.7935e+01,\n",
       "         -5.3335e+01,  2.7929e+01, -5.0188e+01, -1.0566e+01,  8.9571e+01,\n",
       "         -1.3424e+02, -3.6763e+01,  2.6904e+01,  2.6746e+01, -6.8715e+01,\n",
       "         -1.1985e+02, -1.5158e+01, -7.5806e+01, -1.1909e+01,  1.0303e+01,\n",
       "         -2.8290e+01,  6.4190e+01,  7.6386e+01, -9.0905e+01,  5.8220e+00,\n",
       "         -1.9991e+01,  2.0311e+00,  6.6936e+00, -1.1354e+01,  8.5672e+01,\n",
       "          3.1756e+01, -3.4103e+01,  1.0608e+02, -6.8720e+01,  9.0409e+01,\n",
       "          3.1006e+01,  3.4758e+01, -4.5118e+00,  1.0278e+02,  3.5872e+01,\n",
       "          3.8199e+01,  1.7078e+01, -2.9918e+01, -1.2388e+02, -8.5593e+01,\n",
       "         -4.9784e+01,  2.5563e+01,  1.0645e+02,  3.7029e+01,  4.1428e+00,\n",
       "         -3.0568e+01,  1.2047e+02,  5.5957e-01,  6.6613e+01,  6.1269e+01,\n",
       "         -1.1416e+01,  2.2035e+01, -3.5971e+01, -1.6876e+01, -1.6440e+01,\n",
       "         -1.5190e+01, -9.7815e+00, -8.7429e+00,  1.4688e+01,  2.7742e+01,\n",
       "          5.5926e+01,  1.2721e+02,  1.8084e+00,  1.8164e+01,  2.8831e+00,\n",
       "          6.6953e+01, -3.6224e+01, -6.3201e+01,  8.2562e+00, -2.0958e+02,\n",
       "         -6.9974e+01, -8.4267e+00,  6.8540e+01, -4.6306e-01,  4.4498e+01,\n",
       "         -3.1885e+01,  2.1397e+01, -1.6082e+01,  1.0832e+00,  6.0118e+00,\n",
       "         -7.8709e+01, -9.4880e+01,  9.4466e+01,  5.7424e+01, -3.8655e+01,\n",
       "          2.1337e+01,  3.1244e+01, -3.8487e+01,  9.9026e+01,  3.2572e+01,\n",
       "         -4.2623e+01, -2.5182e+03,  4.0676e+01, -7.0653e+01, -5.2857e+01,\n",
       "         -4.8931e+01,  3.2839e+01,  6.6519e+01, -8.4690e+00,  4.4458e+00,\n",
       "         -2.0436e+01, -7.6343e+00, -1.6017e+01,  9.4193e+01, -7.0770e+01,\n",
       "         -5.1615e+01,  4.9339e+01, -3.3707e+01, -7.4959e+01,  3.1823e+01,\n",
       "         -5.4293e+01,  3.5916e+01, -1.8026e+01, -5.7531e+01,  1.0240e+02,\n",
       "          6.7268e+01,  4.4189e+01, -2.4363e+00, -5.3735e+01, -9.2961e+01,\n",
       "          1.2993e+02,  4.3996e+01,  3.5159e+01, -4.1855e+01,  3.9228e+01,\n",
       "          4.2313e+01, -3.5530e+01, -9.9111e+00, -5.6788e+00,  1.0553e+02,\n",
       "          6.7810e+00, -5.9786e+00,  3.9472e-01, -4.5221e+01,  2.2557e+00,\n",
       "          6.6547e+01,  4.1571e+00, -4.9843e+01,  2.9751e+01,  6.3929e+01,\n",
       "         -7.3231e+01,  6.0967e+01, -3.0459e+01,  5.4725e+01, -2.7004e+01,\n",
       "          8.5911e+00, -1.3508e+01,  7.9805e+01, -2.2182e+01, -2.3018e+01,\n",
       "          5.0016e+01, -5.8606e+01,  1.0965e+01, -3.1222e+01,  6.7811e+01,\n",
       "         -3.3888e+01, -1.3158e+02,  4.4655e+01,  6.3709e+01, -5.9749e+01,\n",
       "         -1.9482e+02, -8.7041e-01, -3.8711e+01,  4.7540e+01, -1.4741e+02,\n",
       "         -5.1141e+01, -7.1148e+01,  1.1880e+02, -6.1956e+00,  2.1785e+01,\n",
       "         -5.2320e+01, -7.6599e+01, -7.6518e+01,  4.3173e+01,  1.4991e+01,\n",
       "         -6.1114e+01, -3.4468e+00,  6.9100e+00, -2.6809e+01, -1.2034e+02,\n",
       "          5.5515e+01,  1.2607e+01, -7.0942e+01, -3.3577e+01, -7.3956e+01,\n",
       "          5.7721e+01,  1.0430e+02, -1.3613e+01, -1.9101e+01, -8.1333e+01,\n",
       "          3.3224e+01, -3.1622e+01,  1.6285e+01, -1.3455e+02,  8.4806e+00,\n",
       "         -4.7249e+01,  4.6909e+01, -5.0489e+01,  1.0939e+01,  4.7157e+01,\n",
       "         -1.7716e+00,  4.2356e+01, -1.4138e+01,  3.3308e+01,  5.5508e+01,\n",
       "          7.1233e+01, -3.5375e+00, -1.4681e+02, -5.5914e+01, -8.1592e+00,\n",
       "          1.9370e+01,  4.7886e+01,  7.4217e+01,  7.3146e+01,  1.4521e+01,\n",
       "          5.6708e+01,  5.1028e+00,  8.9700e+01,  1.9658e+01,  9.2783e+01,\n",
       "          7.2455e+00,  1.7630e+01, -4.7039e+01, -3.6197e+01, -5.9747e+00,\n",
       "         -3.2650e+01,  1.7360e+01,  8.8992e+01, -5.7876e+00,  4.0617e+01,\n",
       "          7.4355e+01, -1.0374e+00,  6.2236e+01, -1.0647e+02, -6.8081e+01,\n",
       "          1.4336e+02, -2.7826e+00, -8.2957e+00, -2.8645e+00,  7.7842e+01,\n",
       "          3.0244e+01,  7.7033e+00,  8.5519e+01, -2.7147e+01,  7.7740e+01,\n",
       "         -1.6725e+01,  7.1979e+01,  2.3842e+01,  8.6530e+00,  8.5170e+00,\n",
       "          2.2111e+01, -2.1895e+01, -8.8762e+00, -1.6845e+01, -3.0862e+01,\n",
       "          1.1974e+02, -1.2500e+01, -6.1151e+01,  3.2982e+00,  7.3855e+01,\n",
       "         -2.9248e+01,  5.8160e+01,  3.6505e+01, -3.1443e+01,  6.5210e+01,\n",
       "         -5.6792e+01,  6.2496e+01,  1.1552e+01, -7.8235e+00,  5.3614e+01,\n",
       "          5.4854e+00,  3.1450e+01, -5.3260e+01, -3.8267e+01, -1.2771e+02,\n",
       "          5.7352e+01, -1.9902e+01, -2.0927e+02, -1.0375e+02, -5.5973e+01,\n",
       "          2.7925e+01, -4.7714e+01,  5.7874e+01,  6.4867e+01, -5.9362e+01,\n",
       "         -1.2062e+01, -4.4570e+00, -3.4833e+01, -6.3883e+00, -2.4791e+01,\n",
       "          1.7301e+01,  2.5652e+01,  7.8714e+01, -9.9464e+00,  2.6012e+01,\n",
       "         -3.9254e+01, -5.9209e+01, -2.4939e+01,  7.5600e+01, -7.8083e+01,\n",
       "          3.2038e+01, -3.3137e+01, -4.5380e+01,  7.1825e+01,  1.5415e+01,\n",
       "          2.3999e+00, -1.3817e+02,  8.5010e+00,  8.7524e+01, -1.5914e+01,\n",
       "          4.5287e+01,  6.2370e+01,  2.1991e+01,  1.3673e+01, -2.7580e+01,\n",
       "         -2.9827e+00,  2.6729e+01, -1.1591e+01,  9.4563e+01,  1.6903e+01,\n",
       "          3.1161e+00, -2.7758e+01,  2.1629e+01, -1.5702e+01, -7.9772e+01,\n",
       "          1.6503e+01,  1.1299e+01, -2.1515e+01, -9.5256e+01,  2.2635e+01,\n",
       "         -4.2841e+01, -1.0723e+02, -2.5018e+01, -5.8148e+00, -2.7788e+01,\n",
       "         -9.4555e+01, -1.1735e+01,  1.8214e+02, -7.1663e+01,  1.8552e+01,\n",
       "         -9.3540e+01, -1.4856e+02,  1.4214e+01,  3.0221e+01,  2.2933e+01,\n",
       "          7.2221e+01, -2.8985e+01, -6.2819e+01, -2.7837e+01,  8.3745e+00,\n",
       "          9.4338e+01,  5.2704e+01, -3.1204e+01,  2.2469e+01, -1.6729e+01,\n",
       "          4.2350e+01,  7.5738e+00,  5.8672e+01, -1.4735e+01,  9.9326e+01,\n",
       "         -2.3494e+01, -3.1521e+00,  1.4195e+02, -5.3589e+01, -1.2559e+01,\n",
       "          5.2201e+01, -4.9185e+01,  1.5929e+01, -5.4150e+01, -2.2133e+01,\n",
       "         -1.4894e+02,  4.1252e+01, -4.9394e+01, -3.1192e+01, -4.5648e+01,\n",
       "         -6.8027e+01, -3.3289e+00,  7.2700e+00, -1.1295e+01,  4.3899e+01,\n",
       "          1.4052e+01,  1.3899e+01,  9.1015e-01,  5.5149e+01, -1.1283e+02,\n",
       "         -2.6935e+01,  1.2536e+01, -2.2157e+01,  8.9940e+01,  6.7645e+01,\n",
       "         -5.0762e+01, -1.2234e+01,  3.0896e+01,  4.3185e+01, -2.9578e+01,\n",
       "          4.7415e+01,  1.8394e+01,  7.0962e+01,  9.6639e+00, -9.0609e+00,\n",
       "         -1.1217e+02,  1.2657e+02, -2.9871e+01, -3.0401e+01,  8.7114e+00,\n",
       "          2.7541e+01,  4.1672e+01,  1.7820e+01,  2.1750e+01,  2.9505e+01,\n",
       "          3.2681e+01, -1.7476e+01,  5.5166e+01,  3.4561e+01,  1.0477e+02,\n",
       "          2.9853e+01, -1.0215e+02,  5.8220e+00,  4.3573e+00, -1.3960e+01,\n",
       "          7.7088e+00,  7.7094e+01, -6.6180e+00, -4.2709e+00, -1.4481e+02,\n",
       "         -6.8161e+01,  3.1450e+01,  1.0043e+02,  4.2479e+01,  4.4813e+01,\n",
       "          9.9548e+01, -1.0593e+01,  1.0153e+02, -4.3473e+01, -7.7222e+01,\n",
       "          3.2498e+01, -1.0555e+01,  9.4412e+01,  2.6183e+01,  1.5023e+01,\n",
       "         -7.6549e+01, -2.2190e+01,  2.5431e+01,  3.0627e+01, -1.8290e+02,\n",
       "          8.8760e+00, -9.0313e+01,  8.7821e+01,  2.1939e+01, -2.8168e+01,\n",
       "          5.5007e+01, -2.9898e+01,  9.9422e+01,  1.1976e+01,  6.3917e+01,\n",
       "          1.0749e+01,  1.3136e+01,  7.5076e+01, -1.4279e+01,  2.6067e+01,\n",
       "          2.8964e+01,  8.7519e+01,  2.9284e+01,  3.6313e+01, -1.7838e+01,\n",
       "          9.4921e+00, -4.5029e+00,  2.2216e+01,  1.5594e+01,  4.9050e+01,\n",
       "         -3.5628e+01,  5.4996e+01,  5.5351e+01, -2.8016e+01,  3.4296e+01,\n",
       "         -9.3678e+01, -4.0457e+01, -2.3030e+01, -6.6328e+01,  1.1888e+01,\n",
       "         -8.7345e+01, -6.3163e+01,  8.1772e+00,  2.1457e+01,  6.2812e+01,\n",
       "          6.1641e+01,  7.1548e+01, -3.7915e+01, -3.9875e+00,  4.7814e+01,\n",
       "         -4.4955e+01, -3.2216e+01, -6.4092e+01,  1.2232e+02, -3.3282e+00,\n",
       "          2.4606e+01, -4.4960e+01,  5.5844e+01,  1.0077e+02,  1.7162e+01,\n",
       "          1.0484e+02, -1.1357e+01,  3.1213e+01,  2.3458e+01,  4.5138e+01,\n",
       "          2.9461e+01,  5.9166e+01, -7.3748e+01,  1.4311e+01, -6.5951e+01,\n",
       "         -2.5799e+01,  2.8306e+01,  9.0410e+01,  1.1794e+01,  1.0044e+02,\n",
       "          6.5607e+01,  6.4192e+00, -3.2565e+01,  3.4913e+01, -1.8112e+01,\n",
       "         -3.2785e+01, -9.4875e+01, -1.1086e+02,  5.6049e+01,  7.0902e+01,\n",
       "         -6.1625e+01,  5.1290e+00,  5.1160e+01,  1.3707e+01,  3.4006e+01,\n",
       "          1.4479e+01,  4.0862e+01,  2.9371e+01,  2.9897e+01,  6.9398e+01,\n",
       "          8.0202e+01,  4.2108e+00, -1.2336e+01,  2.7643e+01, -4.2854e+01,\n",
       "         -2.5117e+01,  2.2971e+01, -2.9894e+01, -2.4572e+01,  7.7202e+00,\n",
       "          6.8241e+01, -4.0590e+01, -4.7987e+01,  5.4209e+01, -2.1105e+01,\n",
       "          4.7510e+01, -1.5974e+01, -3.3459e+01,  3.9861e+01, -5.4791e+01,\n",
       "         -3.8464e+01,  1.4008e+02,  1.5097e+02, -7.1160e+01,  4.2940e+01,\n",
       "         -2.0056e+01,  6.6014e+00, -4.3568e+01,  4.4796e+00, -9.4151e+01,\n",
       "          2.1579e+01,  2.1897e+01,  3.5422e+01, -5.5565e-01,  5.0536e+00,\n",
       "          6.5255e+01,  5.9026e+01, -2.3345e+01, -2.3027e+00, -5.9094e+00,\n",
       "         -3.8018e+01,  2.4777e+01,  2.1092e+01,  1.6766e+01,  3.9666e+01,\n",
       "         -8.0887e+01, -1.8393e+00, -1.0311e+02, -1.8489e+01,  1.5921e+01,\n",
       "         -3.6076e+00,  7.9168e+01,  3.3094e+01,  2.0574e-01, -4.7314e+00,\n",
       "          2.9268e+01,  6.5123e+01, -2.0053e+01, -1.4636e+02,  5.6351e+01,\n",
       "         -8.8578e+01,  8.8923e+00,  1.4502e+01, -4.8800e+00,  9.0794e+01,\n",
       "          2.1478e+01, -1.3196e+02, -1.3651e+02, -2.5636e+01,  1.7520e+02,\n",
       "         -9.8640e+01,  7.8448e+01,  1.0286e+02]),\n",
       " 'char_start': 75,\n",
       " 'char_end': 80}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_list[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827822b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.9134914875030518\n"
     ]
    }
   ],
   "source": [
    "# comparing epic across both vectors\n",
    "\n",
    "import torch.nn.functional as F\n",
    "vec1 = example_list[13][\"vector\"]\n",
    "vec2 = cleaned_list[14][\"vector\"]\n",
    "\n",
    "cos_sim = F.cosine_similarity(vec1.unsqueeze(0), vec2.unsqueeze(0)).item()\n",
    "print(\"Cosine similarity:\", cos_sim)\n",
    "\n",
    "### only 90% SIMILARITY, so bad OCR vs good OCR is leading to 10% difference in DIRECTION for the embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e0e5cd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "TARGET_COLLECTIONS = {\"Literary\", \"Linguistic\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8c8f8c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ngupta1/Desktop/GitHub/PPA-Word-Embeddings'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2c2aec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TARGET_COLLECTIONS = {\"Literary\", \"Linguistic\"}\n",
    "with open(\"Data/ppa_corpus_2025-02-03_1308/ppa_metadata.json\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "metadata_index = {\n",
    "    entry[\"work_id\"]: entry for entry in metadata\n",
    "    if \"collections\" in entry and any(c in TARGET_COLLECTIONS for c in entry[\"collections\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "52bd2f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [04:08,  4.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process corpus\n",
    "\n",
    "def is_semantically_meaningful(token):\n",
    "    token = token.lower()\n",
    "    STOPWORDS = {\n",
    "        \"the\", \"and\", \"for\", \"but\", \"with\", \"that\", \"this\", \"from\", \"not\",\n",
    "        \"you\", \"are\", \"was\", \"were\", \"have\", \"has\", \"had\", \"she\", \"he\", \"they\",\n",
    "        \"his\", \"her\", \"its\", \"our\", \"their\", \"will\", \"would\", \"can\", \"could\"\n",
    "    }\n",
    "    return (\n",
    "        token.isalpha() and\n",
    "        len(token) > 2 and\n",
    "        token not in STOPWORDS\n",
    "    )\n",
    "\n",
    "output = []\n",
    "\n",
    "\n",
    "# Save output\n",
    "\n",
    "for example in tqdm(islice(page_iter(\"Data/ppa_corpus_2025-02-03_1308/ppa_pages.jsonl.gz\"), 1000)):\n",
    "        text = example.get(\"text\")\n",
    "        pid = example.get(\"id\")\n",
    "        wid = example.get(\"work_id\")\n",
    "\n",
    "        if not text or wid not in metadata_index:\n",
    "            continue\n",
    "        meta = metadata_index[wid]\n",
    "        pub_year = meta.get(\"pub_year\")\n",
    "        collections = meta.get(\"collections\")\n",
    "\n",
    "        for word_info in extract_usage_representations(text, tokenizer, model, device=DEVICE):\n",
    "            word = word_info['word']\n",
    "            output.append({\n",
    "                \"word\": word,\n",
    "                \"usage_vector\": word_info[\"vector\"].tolist(),\n",
    "                \"char_start\": word_info[\"char_start\"],\n",
    "                \"char_end\": word_info[\"char_end\"], \n",
    "                \"id\": pid,\n",
    "                \"work_id\": wid,\n",
    "                })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### read in output\n",
    "## filter down to target words\n",
    "## remerge \n",
    "## BGAK \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdb6360e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(output[0]['usage_vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ccc8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d04f9c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '[1]', 'tags': ['dedication'], 'id': 'A01224.1', 'order': 1, 'text': '\\nTo the Right excellent and most honorable Ladie, the Ladie Marie, Countesse of Pembroke.\\nVOi, pia nympha, tuum, quem tolse la morte, Philippū,\\nAEdentem llenas coelestis melle palabras.\\nItalicum lumen, flowre of Fraunce, splendor Iberus,\\nItalicus Tasso, French Salust, Boscan Iberus,\\n〈 in non-Latin alphabet 〉 Virgil, 〈 in non-Latin alphabet 〉,\\nGreekish Homer, tanto lati iunguntur 〈 in non-Latin alphabet 〉.\\nYour Honors most affectionate. Abraham Fraunce.\\n\\n\\n\\n\\n', 'work_id': 'A01224'}\n",
      "{'label': '[10]', 'tags': ['book'], 'id': 'A01224.10', 'order': 10, 'text': \"Boscan 3. Booke.\\nLos altares delante estauan puestos,\\nArdiendo encima d'ellos toda Arabia.\\n\\n\\nCap. 5. Of the Metonymia of the adiunct.\\nA Metonymia of the adiunct, is, when by the adiunct we expresse the subiect. So the names of vertues & vices are vsed for vertuous or vicious men: the signe for the thing which it doth signifie: the adiūct of the time for the things subiect, &c.\\nHomer. N. Il. \\n〈 in non-Latin alphabet 〉.\\n\\nSo in \\n6. Odyss.\\n〈 in non-Latin alphabet 〉.\\n\\nfor Alcinous himself.\\nV. Ae. 1. \\nQuis genus Aeneadûm, quis Troiae nesciat vrbē,\\nVirtutésque, virósque, & tanti incendia belli?\\n\\nAeneid. 1. \\nHaud aliter puppés{que} tua, pubés{que} tuorum, &c.\\nAeneid. 1. \\nAspera tum positis mitescent saecula bellis.\\n\\nGeorg. 1. \\nImpia{que} aternam timuerunt saecula noctem.\\n\\nAglog. 1. \\n—libet Partho torquere Cydonia cornis Spicula.\\n\\nSir Philip Sidney 5. Musidorus to Euarchus.\\nHowsoeuer it bee, my death shall triumph ouer thy crueltie.\\n\\n\", 'work_id': 'A01224'}\n",
      "{'label': '[100]', 'tags': ['book'], 'id': 'A01224.100', 'order': 100, 'text': '\\n〈 in non-Latin alphabet 〉.\\n\\n〈 in non-Latin alphabet 〉.\\nThis is the Prosopopoeia of Peleus, which is thus left off;\\n〈 in non-Latin alphabet 〉.\\nVirgil. Aeneid. 1. Of Aeneas.\\n—& dictis moerentia pectora mulcet.\\nO socij (neque enim ignari sumus antemalorum)\\nO passi grauiora! dabit Deus his quoque finem.\\nVos & Scyllaeam rabiem, penitus{que} sonantes\\nAccestis scopulos: vos & Cyclopea saxa\\nExperti: reuocate animos, moestùmque timorem\\nMittite: forsan & haec olim meminisse iuuabit.\\nPer varios casus, per tot discrimina rerum\\nTendimus in Latium, sedes vbi fata quietas\\nOstendunt: illie fas regna resurgere Troiae:\\nDurate, & vosinet rebus seruate secundis.\\nTalia voce refert, curisque ingentibus ager,\\nSpem vultu simulat, premit altum corde dolorem.\\nSo of Achemenides 3. Aeneid.\\nCùm subitò è syluis, macie confecta suprema\\nIgnoti noua forma viri, miserandáque cultu\\nProcedit, suppléxque manus ad littora tendit.\\nRespicimus: dira illuuies, immissa{que} barba,\\nConsertum tegimen spinis: at caetera Graius,\\nEt quondam patrijs ad Troiam missus in armis.\\nIs{que} vbi Dardanios habitus & Troia vidit\\nArma procul: paulùm aspectu conterritus haesit,\\nContinuitque gradum: mox sese ad littora praeceps\\nCum fletu, precibúsque tulit. Per sidera testor,\\nPer superos, atque hoc coeli spirabile lumen,\\nTollite me Teucri, quascunque abducite terras.\\nHoc sat erit. Scio me Danais è classibus vnum,\\n', 'work_id': 'A01224'}\n",
      "{'label': '[101]', 'tags': ['book'], 'id': 'A01224.101', 'order': 101, 'text': '\\nEt bello Iliacos fateor perijsse penates:\\nPro quo, sisceleria tanta est iniuria nostra,\\nSpargite me influctus, vastóque immergite ponto.\\nSi pereo, hominum manibus perijsse iuuabit.\\nDixerat, & genua amplexus genibisque volutans\\nHarebat.\\n\\nSir Philip Sydney 1. Of Musidorus clad in shepheards weedes.\\nShe might percetue a farre off one cōming towards her in the apparaile of a shepheard, with his armes hanging down, going a kinde of languishing pace, with his eyes sometimes cast vp to heauen, as though his fancie straue to moūt vp higher; somtimes thrown down to the groūd, as if the earth could not beare the burden of his paines: at length she heard him with a lamentable tune sing these few verses:\\n\\nCome shepheards weedes, become your masters mind,\\nYeeld outward shew, what inward change he tries,\\nNor be abasht, since such a guest you find,\\nWhose strongest hope in your weake comfort lies.\\nCome shepheards weeds, attend my wofull cries,\\nDisuse your selues from sweet Menalcaes voyce,\\nFor other be those tunes which sorowe ties,\\nFrom those cleare notes which freelie may reioyce.\\nThen powre out plaints, and in one word say this,\\nHelples his plaint who spoyles himselfe of blisse.\\n\\nAnd hauing ended, she might see him strike himselfe vppon the breast, vttering\\n\\n\\n', 'work_id': 'A01224'}\n",
      "{'label': '[102]', 'tags': ['book'], 'id': 'A01224.102', 'order': 102, 'text': \"\\nthese words. O miserable wretch, whether doothy destinies guide thee?\\n\\nTorquat. Tass. 3. Godfrey to his Souldiers.\\nIl saggio Capitan, che l'ardimento\\nSolite loro, in essi hor non comprende:\\nCerca con lieto volto, è conparole\\nCome gli rassicuri, è riconsole.\\nO per mille perigli, è mille affanni\\nMeco passati in quelle parti, e'n queste,\\nCampion di Dio, ch'à ristorare i danni\\nDe la Christiana sua fede nasceste,\\nVoi, che l'arme di Persia, è i Greci ing anni\\nE i monti, è i mari, el verno, è le tempeste,\\nDe la fame i disagi, & de la sete\\nSuperaste: voi dunque hora temete?\\nDun{que} il signor, che m'indirizza, è moue\\nGià conosciuto in caso assai più •io,\\nNon v'assicura? quasi hor volga altroue\\nLa man de la clementia, e'lguardo pio.\\nTosto vn difia, che rimembrar vi gious\\nGli scorsi affanni, è sciorre i voti à dio.\\nHor durate magnanimi, è voi stessi\\nSerbate, prego, 〈◊〉 i prosperi successi.\\nCon queste detti le smarxite menti\\nConsola, è con sereno è lieto aspetto:\\nMà preme mille cure egro è dolenti\\nAltamente riposte in mezz' alpetto.\\n\\nSalust. Iudith. 2, God to Moses.\\n\\nVniour que cet Hebrieu sur Oreb menoit paitre\\nLes laineuses brebis de son beau pere Ietre,\\nIl voit, tout effraié comme vn rougeatre feu\\nSans amorce s'espend en vn halier toufu:\\nD'où sort auec grand bruit vne telle parole\\n\\n\\n\", 'work_id': 'A01224'}\n",
      "{'label': '[103]', 'tags': ['book'], 'id': 'A01224.103', 'order': 103, 'text': \"\\nQui soudain fait trembler & la terre & le polo.\\n\\n\\nIesuis cil qui seul est, seul fut, & seul sera:\\nCil qui de rien fit tout, & qui fort reduira\\nS'il veut, le tout en tien. Iesuis le grand, le iuste,\\nLe beau, le bon, le saint, dont la dextre robuste\\nBalance l'vniuers. Iesuis le tout-puissant,\\nQu' Abram seul adoroit: qui d'vn fleau puissant\\nDestrui mes enemis, & qui benin, fa grace\\nA ceux qui m'ont pour Dieu voire à toute leur race.\\nSi doncques mon vouloir. Va-ten, despeche toi,\\nFai scauoir de ma part à ce profane roy,\\nQui tient les tours de Memphe, & la grasse campagne,\\nQue le Nil debordé de son flot riche bagne,\\nQu'il assranchisse Isac. Et de peur que le roy\\nIncredule ne mettent en doute ton enuoy,\\nIeveus que sur les fleurs ta houlette alongee,\\nPour confirmer ta charge, en serpent soit changee.\\n\\n\\nBoscan. 3. of Hero.\\n\\nElla mouida entonces con mas san̄a,\\nNise dexolleuar por donde el quiso,\\nNisobre el manto le sufrio la mano,\\nMas buelta sobre si congraue gesto,\\nSemeiantes palabras d'estas dizo:\\n\\n\\nO hombre que veniste por mal tuyo\\nA este templo, a des lustrar mi houra,\\nSin entender quan gran locura emprendes.\\nNo sabes tu que soy sierua de Venus,\\nY virgin, y por virgen que la siruo?\\nNo sabes tulos hombres de mi sangre\\nQue ti castigaran, si saben esto?\\n\\n\\n\", 'work_id': 'A01224'}\n"
     ]
    }
   ],
   "source": [
    "for i, example in enumerate(page_iter(\"Data/ppa_corpus_2025-02-03_1308/ppa_pages.jsonl.gz\")):\n",
    "    print(example)\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b0ff9666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248308639\n"
     ]
    }
   ],
   "source": [
    "def is_semantically_meaningful(token):\n",
    "    token = token.lower()\n",
    "    STOPWORDS = {\n",
    "        \"the\", \"and\", \"for\", \"but\", \"with\", \"that\", \"this\", \"from\", \"not\",\n",
    "        \"you\", \"are\", \"was\", \"were\", \"have\", \"has\", \"had\", \"she\", \"he\", \"they\",\n",
    "        \"his\", \"her\", \"its\", \"our\", \"their\", \"will\", \"would\", \"can\", \"could\"\n",
    "    }\n",
    "    return (\n",
    "        token.isalpha() and\n",
    "        len(token) > 2 and\n",
    "        token not in STOPWORDS\n",
    "    )\n",
    "\n",
    "\n",
    "total_semantic_word_count = 0\n",
    "for i, line in enumerate(page_iter(\"Data/ppa_corpus_2025-02-03_1308/ppa_pages.jsonl.gz\")):\n",
    "    if 'text' in line and line['text']:\n",
    "        tokens = line['text'].split()\n",
    "        for token in tokens:\n",
    "            if is_semantically_meaningful(token):\n",
    "                total_semantic_word_count += 1\n",
    "\n",
    "print(total_semantic_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1236df2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated JSONL size: 3618.22 MB\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "size_bytes = sum(len(json.dumps(item).encode(\"utf-8\")) + 1 for item in output)  # +1 for newline\n",
    "size_mb = size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Estimated JSONL size: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fea05cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1939462"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pages = 0\n",
    "for i, line in enumerate(page_iter(\"Data/ppa_corpus_2025-02-03_1308/ppa_pages.jsonl.gz\")):\n",
    "    if line['work_id'] in metadata_index:\n",
    "        all_pages += 1\n",
    "\n",
    "all_pages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
