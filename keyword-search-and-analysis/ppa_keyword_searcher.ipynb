{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gh-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, json, pathlib, itertools, pandas as pd, re, ast\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_DIR = pathlib.Path('../Data') / 'ppa_corpus_2025-02-03_1308'\n",
    "PAGES_FILE = DATA_DIR / 'ppa_pages.jsonl.gz'\n",
    "META_CSV   = DATA_DIR / 'ppa_metadata.csv'\n",
    "\n",
    "metadata_df = pd.read_csv(META_CSV, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gh-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spelling variants for each poetic form\n",
    "# longer multi-word patterns come first so regex matches them before shorter ones (e.g. 'pindaric ode' before 'ode')\n",
    "POETIC_FORMS_KEYWORDS = {\n",
    "    'Ballad': ['balad', 'ballade', 'ballad'],\n",
    "    'Ghazal': ['ghazel', 'ghazels', 'guzel', 'guzels', 'gazal', 'gazals', 'ghazul', 'ghazuls', 'ghazal', 'ghazals'],\n",
    "    'Haiku': ['haicu', 'haïku', 'hiaku', 'hokku', 'haiku'],\n",
    "    'Limerick': ['limeric', 'limerik', 'limerick'],\n",
    "    'Pantoum': ['pantun', 'pantoun', 'pantowm', 'pantoum'],\n",
    "    'Sestina': ['sestine', 'sestena', 'sistina', 'sestina'],\n",
    "    'Sonnet': ['sonet', 'sonnete', 'sonnette', 'sonneta', 'sonnetto', 'sonnet'],\n",
    "    'Villanelle': ['villanella', 'villanell', 'villanela', 'Villǎnelle', 'villanel', 'villanelle'],\n",
    "    'Blank Verse': ['blank verse', 'blanke verse', 'blanck verse', 'blancke verse'],\n",
    "    'Free Verse': ['free verse', 'vers libre', 'freee verse'],\n",
    "    'Common Measure': ['common meter', 'common metre', 'common measure', 'common-measure'],\n",
    "    'Ars Poetica': ['ars poetica', 'ars poeticæ'],\n",
    "    'Aubade': ['aubade', 'aubad', 'aubadee', 'aubadé'],\n",
    "    'Concrete Poetry': ['concrete poetry', 'concrete poem', 'pattern poem', 'pattern poetry'],\n",
    "    'Dramatic Monologue': ['dramatic monologue', 'dramatic soliloquy'],\n",
    "    'Ekphrasis': ['ekphrasis', 'ecphrasis', 'ekphrastic'],\n",
    "    'Elegy': ['elegy', 'elegie', 'elogy', 'elegiac'],\n",
    "    'Ode': ['pindaric ode', 'pindarick ode', 'horatian ode', 'pindaric', 'odes', 'ode'],\n",
    "    'Hymn': ['hymn', 'hynm'],\n",
    "    'Rondeau': ['rondeau'],\n",
    "    'Pastoral': ['pastoral', 'pastorel', 'pastorall'],\n",
    "    'Prose Poem': ['prose poem', 'prose poetry', 'prose-poetry'],\n",
    "    'Verse Novel': ['verse novel', 'novel in verse'],\n",
    "    'Epic': ['epic', 'epics', 'epick', 'epicks'],\n",
    "    \"Ruba'i\": ['rubai', 'rubayat', 'rubaiyat', \"ruba'ee\", \"rubá'í\"],\n",
    "    'Song': ['song', 'songs'],\n",
    "    'Lyric': ['lyric']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gh-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the top 5000 most common corpus words (generated by most-frequently-occuring-words.ipynb)\n",
    "with open('../Data/top_5000_words_list.txt', 'r') as f:\n",
    "    top_5000_words = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gh-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge poetic forms + top-5000 into one dict, tagging each entry with its source\n",
    "def prepare_keyword_dict_with_sources(poetic_forms, top_5000):\n",
    "    combined_keywords = {}\n",
    "    keyword_sources = {}\n",
    "\n",
    "    for form, spellings in poetic_forms.items():\n",
    "        combined_keywords[form] = spellings\n",
    "        keyword_sources[form] = 'poetic_forms'\n",
    "\n",
    "    for word in top_5000:\n",
    "        combined_keywords[word] = [word]\n",
    "        keyword_sources[word] = 'top_5000'\n",
    "\n",
    "    return combined_keywords, keyword_sources\n",
    "\n",
    "combined_keywords, keyword_sources = prepare_keyword_dict_with_sources(POETIC_FORMS_KEYWORDS, top_5000_words)\n",
    "print(f'Total keywords: {len(combined_keywords)} ({len(POETIC_FORMS_KEYWORDS)} poetic forms + {len(top_5000_words)} top words)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gh-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_iter(pages_file):\n",
    "    # stream pages one at a time\n",
    "    with gzip.open(pages_file, 'rt', encoding='utf-8') as fh:\n",
    "        for line in fh:\n",
    "            yield json.loads(line)\n",
    "\n",
    "def extract_context(text, pos, length, window=300):\n",
    "    # grab 300 chars on either side of a match\n",
    "    start = max(0, pos - window)\n",
    "    end = min(len(text), pos + length + window)\n",
    "    return text[start:end]\n",
    "\n",
    "def compile_patterns(keywords_dict, keyword_sources):\n",
    "    # compile all regexes once upfront\n",
    "    # short words get word boundaries to avoid false positives (e.g. 'ode' inside 'model')\n",
    "    needs_boundaries = {\n",
    "        'ode', 'odes', 'epic', 'epics', 'epick', 'epicks', 'hymn',\n",
    "        'ghazel', 'ghazels', 'guzel', 'guzels', 'gazal', 'gazals',\n",
    "        'ghazul', 'ghazuls', 'ghazal', 'ghazals', 'song', 'songs', 'lyric', 'lay'\n",
    "    }\n",
    "    patterns = {}\n",
    "\n",
    "    for form, spellings in keywords_dict.items():\n",
    "        pats = []\n",
    "        source = keyword_sources[form]\n",
    "        for spelling in spellings:\n",
    "            clean = spelling.strip()\n",
    "            if source == 'top_5000' or clean.lower() in needs_boundaries:\n",
    "                pats.append(r'\\b' + re.escape(clean) + r'\\b')\n",
    "            else:\n",
    "                pats.append(re.escape(clean))\n",
    "        patterns[form] = re.compile('|'.join(pats), re.IGNORECASE)\n",
    "\n",
    "    return patterns\n",
    "\n",
    "def find_matches(page, patterns, keyword_sources):\n",
    "    # search a single page for all keywords, return one row per keyword hit\n",
    "    text = page.get('text', '')\n",
    "    matches = []\n",
    "\n",
    "    for form, pattern in patterns.items():\n",
    "        found = list(pattern.finditer(text))\n",
    "        if found:\n",
    "            source = keyword_sources[form]\n",
    "            matches.append({\n",
    "                'page_id': page['id'],\n",
    "                'work_id': page['work_id'],\n",
    "                'order': page['order'],\n",
    "                'poetic_form': form if source == 'poetic_forms' else None,\n",
    "                'top_5000_word': form if source == 'top_5000' else None,\n",
    "                'keyword_source': source,\n",
    "                'tags': page.get('tags'),\n",
    "                'counts': len(found),\n",
    "                'contexts': [extract_context(text, m.start(), len(m.group())) for m in found],\n",
    "                'page_text': text,\n",
    "                'spelling': list(set(m.group().lower() for m in found))\n",
    "            })\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gh-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_keywords(pages_file, keywords_dict, keyword_sources, metadata_df=None,\n",
    "                    batch_size=10000, max_pages=None, output_dir='keyword_results'):\n",
    "    # main pipeline: filter to lit/ling works, search every page, write batched CSVs, then combine\n",
    "    # set max_pages to a small number (e.g. 1000) for a quick test run\n",
    "    import os\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    lit_ling_work_ids = set()\n",
    "    meta_lookup = None\n",
    "\n",
    "    if metadata_df is not None:\n",
    "        meta_lookup = metadata_df.set_index('work_id')\n",
    "        meta_lookup['collections_parsed'] = meta_lookup['collections'].apply(ast.literal_eval)\n",
    "        mask = meta_lookup['collections_parsed'].isin([\n",
    "            ['Literary'], ['Linguistic'], ['Linguistic', 'Literary'], ['Literary', 'Linguistic']\n",
    "        ])\n",
    "        lit_ling_work_ids = set(meta_lookup[mask].index)\n",
    "        meta_dict = meta_lookup.to_dict('index')\n",
    "        print(f'Found {len(lit_ling_work_ids):,} Literary/Linguistic works')\n",
    "\n",
    "    patterns = compile_patterns(keywords_dict, keyword_sources)\n",
    "    pages_processed = 0\n",
    "    matched_works = set()\n",
    "    pages_iter = page_iter(pages_file)\n",
    "    csv_batch_num = 0\n",
    "    batch_results = []\n",
    "\n",
    "    # count total pages for the progress bar (~30s on the full corpus)\n",
    "    with gzip.open(pages_file, 'rt') as f:\n",
    "        total_pages = max_pages or sum(1 for _ in f)\n",
    "\n",
    "    with tqdm(total=total_pages, desc='Searching', unit='pages') as pbar:\n",
    "        while True:\n",
    "            batch = list(itertools.islice(pages_iter, batch_size))\n",
    "            if not batch:\n",
    "                break\n",
    "\n",
    "            batch_processed = 0\n",
    "            for page in batch:\n",
    "                if max_pages and pages_processed >= max_pages:\n",
    "                    break\n",
    "                pages_processed += 1\n",
    "                batch_processed += 1\n",
    "                work_id = page['work_id']\n",
    "\n",
    "                # skip works outside our collection filter\n",
    "                if work_id not in lit_ling_work_ids:\n",
    "                    continue\n",
    "\n",
    "                matches = find_matches(page, patterns, keyword_sources)\n",
    "\n",
    "                if matches:\n",
    "                    matched_works.add(work_id)\n",
    "                    if meta_lookup is not None and work_id in meta_dict:\n",
    "                        for match in matches:\n",
    "                            match.update(meta_dict[work_id])\n",
    "                    batch_results.extend(matches)\n",
    "\n",
    "            # flush to disk after each batch\n",
    "            if batch_results:\n",
    "                df_batch = pd.DataFrame(batch_results)\n",
    "                csv_file = os.path.join(output_dir, f'results_batch_{csv_batch_num:04d}.csv')\n",
    "                df_batch.to_csv(csv_file, index=False)\n",
    "                print(f'\\nBatch {csv_batch_num}: {len(df_batch)} rows -> {csv_file}')\n",
    "                batch_results = []\n",
    "                csv_batch_num += 1\n",
    "\n",
    "            pbar.update(batch_processed)\n",
    "            if max_pages and pages_processed >= max_pages:\n",
    "                break\n",
    "\n",
    "    # stitch all the batch files into one\n",
    "    print('\\nCombining batches...')\n",
    "    all_dfs = []\n",
    "    for i in range(csv_batch_num + 1):\n",
    "        csv_file = os.path.join(output_dir, f'results_batch_{i:04d}.csv')\n",
    "        if os.path.exists(csv_file):\n",
    "            all_dfs.append(pd.read_csv(csv_file))\n",
    "\n",
    "    if all_dfs:\n",
    "        final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        final_file = os.path.join(output_dir, 'results_combined.csv')\n",
    "        final_df.to_csv(final_file, index=False)\n",
    "        print(f'Done — {len(final_df):,} rows across {final_df[\"work_id\"].nunique():,} works -> {final_file}')\n",
    "        return final_df\n",
    "\n",
    "    return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gh-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search_keywords(PAGES_FILE, combined_keywords, keyword_sources, metadata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gh-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5899b327",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('ppa_keyword_db.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
